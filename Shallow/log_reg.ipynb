{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- __Model info:__\n",
    "    - features:\n",
    "        - contains negative words (T/F)\n",
    "        - contains offensive words (T/F)\n",
    "        - sentiment (+, -, 0)\n",
    "        - contains emoji\n",
    "    - model: Logistic Regression\n",
    "    - max acc: 80% counter with ngrams(1, 3)\n",
    "    \n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(['stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              tweet  \\\n",
       "0           0  86426  @USER She should ask a few native Americans wh...   \n",
       "1           1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...   \n",
       "2           2  16820  Amazon is investigating Chinese employees who ...   \n",
       "3           3  62688  @USER Someone should'veTaken\" this piece of sh...   \n",
       "4           4  43605  @USER @USER Obama wanted liberals &amp; illega...   \n",
       "5           5  97670                  @USER Liberals are all Kookoo !!!   \n",
       "6           6  77444                   @USER @USER Oh noes! Tough shit.   \n",
       "7           7  52415  @USER was literally just talking about this lo...   \n",
       "8           8  45157                         @USER Buy more icecream!!!   \n",
       "9           9  13384  @USER Canada doesn’t need another CUCK! We alr...   \n",
       "\n",
       "  subtask_a  label_a  \n",
       "0       OFF        1  \n",
       "1       OFF        1  \n",
       "2       NOT        0  \n",
       "3       OFF        1  \n",
       "4       NOT        0  \n",
       "5       OFF        1  \n",
       "6       OFF        1  \n",
       "7       OFF        1  \n",
       "8       NOT        0  \n",
       "9       OFF        1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_DF = pd.read_csv('../Dataset-OLID/OLIDv1.0/data_subtask_a.csv')\n",
    "tweets_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27014</td>\n",
       "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>30530</td>\n",
       "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13876</td>\n",
       "      <td>#Watching #Boomer getting the news that she is...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              tweet  \\\n",
       "0           0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   \n",
       "1           1  27014  #ConstitutionDay is revered by Conservatives, ...   \n",
       "2           2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   \n",
       "3           3  13876  #Watching #Boomer getting the news that she is...   \n",
       "4           4  60133  #NoPasaran: Unity demo to oppose the far-right...   \n",
       "\n",
       "  subtask_a  label_a  \n",
       "0       OFF        1  \n",
       "1       NOT        0  \n",
       "2       NOT        0  \n",
       "3       NOT        0  \n",
       "4       OFF        1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test_DF = pd.read_csv('../Dataset-OLID/OLIDv1.0/test_data_subtask_a.csv')\n",
    "tweets_test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5688</td>\n",
       "      <td>POS</td>\n",
       "      <td>@Janet_Reid  I need help getting my book to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2270</td>\n",
       "      <td>POS</td>\n",
       "      <td>I had a snicker doodle for breakfast. #adulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6452</td>\n",
       "      <td>POS</td>\n",
       "      <td>I slept good 2day I barely sleep any other day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5692</td>\n",
       "      <td>NEG</td>\n",
       "      <td>I can’t believe hit folk singer Charles Manson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6924</td>\n",
       "      <td>POS</td>\n",
       "      <td>@EA $60 triple A title with microtransactions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id sentiment                                              tweet\n",
       "0  5688       POS  @Janet_Reid  I need help getting my book to ma...\n",
       "1  2270       POS    I had a snicker doodle for breakfast. #adulting\n",
       "2  6452       POS  I slept good 2day I barely sleep any other day...\n",
       "3  5692       NEG  I can’t believe hit folk singer Charles Manson...\n",
       "4  6924       POS  @EA $60 triple A title with microtransactions ..."
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test_DF = pd.read_csv('../Dataset-TSA/sentiment500.csv')\n",
    "tweets_test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5688</td>\n",
       "      <td>POS</td>\n",
       "      <td>@Janet_Reid  I need help getting my book to ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2270</td>\n",
       "      <td>POS</td>\n",
       "      <td>I had a snicker doodle for breakfast. #adulting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6452</td>\n",
       "      <td>POS</td>\n",
       "      <td>I slept good 2day I barely sleep any other day...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5692</td>\n",
       "      <td>NEG</td>\n",
       "      <td>I can’t believe hit folk singer Charles Manson...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6924</td>\n",
       "      <td>POS</td>\n",
       "      <td>@EA $60 triple A title with microtransactions ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id sentiment                                              tweet  label\n",
       "0  5688       POS  @Janet_Reid  I need help getting my book to ma...      0\n",
       "1  2270       POS    I had a snicker doodle for breakfast. #adulting      0\n",
       "2  6452       POS  I slept good 2day I barely sleep any other day...      0\n",
       "3  5692       NEG  I can’t believe hit folk singer Charles Manson...      1\n",
       "4  6924       POS  @EA $60 triple A title with microtransactions ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test_DF = pd.read_csv('../Dataset-MTSA/tweets_test.tsv', sep=\"\\t\")\n",
    "tweets_test_DF[\"label\"] = tweets_test_DF[\"sentiment\"].apply(lambda x: 1 if x == \"NEG\" else 0)\n",
    "tweets_test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1964</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Tried to eat some chicken tacos after being ve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2128</td>\n",
       "      <td>NEG</td>\n",
       "      <td>Idk what’s messing up worse this COD WWII, U-v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3332</td>\n",
       "      <td>NOT</td>\n",
       "      <td>I️ need to find me a singer and dancer so we c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253</td>\n",
       "      <td>NOT</td>\n",
       "      <td>Now playing Our Turf Football: Goodell vs. Jon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4466</td>\n",
       "      <td>NEG</td>\n",
       "      <td>I miss football :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id sentiment                                              tweet  label\n",
       "0  1964       NEG  Tried to eat some chicken tacos after being ve...      1\n",
       "1  2128       NEG  Idk what’s messing up worse this COD WWII, U-v...      1\n",
       "2  3332       NOT  I️ need to find me a singer and dancer so we c...      0\n",
       "3   253       NOT  Now playing Our Turf Football: Goodell vs. Jon...      0\n",
       "4  4466       NEG                                 I miss football :(      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test_DF = pd.read_csv(\"/home/dorotea/Documents/FER/APT/tmp/tweets_50-50.tsv\", sep=\"\\t\")\n",
    "tweets_test_DF[\"label\"] = tweets_test_DF[\"sentiment\"].apply(lambda x: 1 if x == \"NEG\" else 0)\n",
    "tweets_test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_test_DF.to_csv('../Dataset-TSA/sentiment500.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets500 = tweets_test_DF[:500]\n",
    "#tweets500.to_csv('../Dataset-TSA/sentiment500.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweets_test_DF = tweets_test_DF.rename(columns={\"SentimentText\":\"tweet\", \"Sentiment\":\"label_a\", \"ItemID\":\"id\"})\\ntweets_test_DF[\"Unnamed: 0\"] = tweets_test_DF.index\\ntweets_test_DF.to_csv(\\'../Dataset-TSA/sentiment.csv\\', encoding=\\'utf-8\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing new data\n",
    "'''\n",
    "tweets_test_DF = tweets_test_DF.rename(columns={\"SentimentText\":\"tweet\", \"Sentiment\":\"label_a\", \"ItemID\":\"id\"})\n",
    "tweets_test_DF[\"Unnamed: 0\"] = tweets_test_DF.index\n",
    "tweets_test_DF[\"label_a\"] = tweets_test_DF[\"label_a\"].apply(lambda x: int(not x))\n",
    "tweets_test_DF = tweets_test_DF.drop(columns=[\"Unnamed: 0.1\"], axis=1)\n",
    "tweets_test_DF[\"subtask_a\"] = tweets_test_DF[\"label_a\"].apply(lambda x: True if x else False)\n",
    "tweets_test_DF.to_csv('../Dataset-TSA/sentiment.csv', encoding='utf-8')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import PorterStemmer\\nfrom nltk.tokenize import WordPunctTokenizer\\nfrom nltk.collocations import BigramCollocationFinder\\nfrom nltk.metrics import BigramAssocMeasures\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessery columns\n",
    "tweets_DF = tweets_DF.rename(columns = {'label_a':'label'})\n",
    "tweets_DF = tweets_DF.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "\n",
    "tweets_test_DF = tweets_test_DF.rename(columns = {'label_a':'label'})\n",
    "tweets_test_DF = tweets_test_DF.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------\n",
    "\n",
    "Removing @USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_USER = re.compile(\"@USER\")\n",
    "\n",
    "def remove_user(tweet):\n",
    "    return REMOVE_USER.sub(\"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['tweet'] = tweets_DF['tweet'].apply(remove_user)\n",
    "tweets_test_DF['tweet'] = tweets_test_DF['tweet'].apply(remove_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "Extracting emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['emoji'] = tweets_DF['tweet'].apply(split_count)\n",
    "tweets_test_DF['emoji'] = tweets_test_DF['tweet'].apply(split_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return [score['neg'], score['neu'], score['pos'], score['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['sentiment'] = tweets_DF['tweet'].apply(sentiment_analyzer_scores)\n",
    "tweets_DF['negative_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[0])\n",
    "tweets_DF['neutral_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[1])\n",
    "tweets_DF['positive_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[2])\n",
    "tweets_DF['compound'] = tweets_DF['sentiment'].apply(lambda row: row[3])\n",
    "\n",
    "tweets_DF = tweets_DF.drop([\"sentiment\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_DF['sentiment'] = tweets_test_DF['tweet'].apply(sentiment_analyzer_scores)\n",
    "tweets_test_DF['negative_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[0])\n",
    "tweets_test_DF['neutral_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[1])\n",
    "tweets_test_DF['positive_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[2])\n",
    "tweets_test_DF['compound'] = tweets_test_DF['sentiment'].apply(lambda row: row[3])\n",
    "\n",
    "tweets_test_DF = tweets_test_DF.drop([\"sentiment\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### C) Positive, negative and offensive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words_file = open(\"negative-words.txt\")\n",
    "negative_words = []\n",
    "\n",
    "positive_words_file = open(\"positive-words.txt\")\n",
    "positive_words = []\n",
    "\n",
    "for line in negative_words_file:\n",
    "    if not str.startswith(line, \";\"):\n",
    "        negative_words.append(line.split(\"\\n\")[0])\n",
    "        \n",
    "negative_words = negative_words[1:]\n",
    "\n",
    "for line in positive_words_file:\n",
    "    if not str.startswith(line, \";\"):\n",
    "        positive_words.append(line.split(\"\\n\")[0])\n",
    "        \n",
    "positive_words = positive_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_negative_words(tweet):\n",
    "    tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "    if any(word in negative_words for word in words):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_for_positive_words(tweet):\n",
    "    tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "    if any(word in positive_words for word in words):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['negative_words'] = tweets_DF['tweet'].apply(check_for_negative_words)\n",
    "tweets_test_DF['negative_words'] = tweets_test_DF['tweet'].apply(check_for_negative_words)\n",
    "\n",
    "tweets_DF['positive_words'] = tweets_DF['tweet'].apply(check_for_positive_words)\n",
    "tweets_test_DF['positive_words'] = tweets_test_DF['tweet'].apply(check_for_positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offensive_words_file = open(\"facebook_bad_words.txt\")\n",
    "offensive_words = []\n",
    "\n",
    "for line in offensive_words_file:\n",
    "    offensive_words.append(line.strip())\n",
    "        \n",
    "offensive_words[0] in \"baldjd 2g1c\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_offensive_words(tweet):\n",
    "    #tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    if any(word in tweet for word in offensive_words):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['offensive_words'] = tweets_DF['tweet'].apply(check_for_offensive_words)\n",
    "tweets_test_DF['offensive_words'] = tweets_test_DF['tweet'].apply(check_for_offensive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### D) Removing stopwords, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #SnowballStemmer('english')\n",
    "   \n",
    "def preprocess_tweet(tweet, remove_stopwords=False):\n",
    "    #remove all non letters\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    removed_nonalphanumeric = regex.sub(' ', tweet)\n",
    "    lowercased_tweet = str.lower(removed_nonalphanumeric)\n",
    "\n",
    "    #remove stopwords\n",
    "    if remove_stopwords:\n",
    "        lowercased_tweet = \" \".join(word if word not in english_stop_words else \"\" for word in lowercased_tweet.split())\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in lowercased_tweet.split()])\n",
    "    return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT \n",
      "\n",
      "amazon is investigating chinese employee who are selling internal data to third party seller looking for an edge in the competitive marketplace url amazon maga kag china tcot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'amazon investigating chinese employee selling internal data third party seller looking edge competitive marketplace url amazon maga kag china tcot'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets_DF['tweet'][2]\n",
    "print(tweet, \"\\n\")\n",
    "print(preprocess_tweet(tweet, False))\n",
    "preprocess_tweet(tweet, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['clean_tweet_with_stopwords'] = tweets_DF['tweet'].apply(preprocess_tweet, False)\n",
    "tweets_DF['clean_tweet_without_stopwords'] = tweets_DF['tweet'].apply(preprocess_tweet, True)\n",
    "\n",
    "tweets_test_DF['clean_tweet_with_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet, False)\n",
    "tweets_test_DF['clean_tweet_without_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, word2vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tweets_DF['clean_tweet_with_stopwords'].values\n",
    "\n",
    "# we need to pass splitted sentences to the model\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "#model.train(corpus, total_examples=1000, epochs=10)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd(sentance):\n",
    "    tokens = sentance.split()\n",
    "    summed_embedding = np.zeros(100)\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            summed_embedding += model.wv[token]\n",
    "        \n",
    "        \n",
    "    return summed_embedding/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz = TweetTokenizer()\n",
    "tweets_test_DF['clean_tweet_with_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet)#(tknz.tokenize).apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['embedding'] = tweets_DF['clean_tweet_with_stopwords'].apply(embedd)\n",
    "tweets_test_DF['embedding'] = tweets_test_DF['clean_tweet_with_stopwords'].apply(embedd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models\n",
    "------------------------------------\n",
    "\n",
    "### 2.1 Model: sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train, test \n",
    "sentiment = tweets_DF[[\"negative_sentiment\", \"negative_words\", \"offensive_words\"]].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "X_test_sentiment = tweets_test_DF[[\"negative_sentiment\", \"negative_words\", \"offensive_words\"]].values\n",
    "y_test_sentiment = tweets_test_DF['label'].values\n",
    "\n",
    "X_train_sentiment, X_val_sentiment, y_train_sentiment, y_val_sentiment = train_test_split(sentiment, y, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7802325581395348\n"
     ]
    }
   ],
   "source": [
    "classifier_sentiment = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "classifier_sentiment.fit(X_train_sentiment, y_train_sentiment)\n",
    "score = classifier_sentiment.score(X_test_sentiment, y_test_sentiment)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47645429362880887"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_sentiment.predict(X_test_sentiment)\n",
    "f1_score(y_test_sentiment, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model : tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['tweet'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['tweet'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "vectorizer_tf = TfidfVectorizer()\n",
    "vectorizer_tf.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_tfidf = vectorizer_tf.transform(tweets_train)\n",
    "X_valid_tfidf = vectorizer_tf.transform(tweets_valid)\n",
    "X_test_tfidf  = vectorizer_tf.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7906976744186046\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = LogisticRegression(max_iter=100)\n",
    "classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "score = classifier_tfidf.score(X_test_tfidf, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44099378881987583"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_tfidf.predict(X_test_tfidf)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model: count_vector, clean data: char-based and word-based\n",
    "- Old data voc size: 19083\n",
    "- New data voc size: 16622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['clean_tweet_without_stopwords'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['clean_tweet_without_stopwords'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "char_vectorizer = CountVectorizer(binary=True, analyzer='char' , ngram_range=(3, 5))\n",
    "char_vectorizer.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_clean = char_vectorizer.transform(tweets_train)\n",
    "X_valid_clean = char_vectorizer.transform(tweets_valid)\n",
    "X_test_clean  = char_vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7662790697674419\n"
     ]
    }
   ],
   "source": [
    "classifier_char = LogisticRegression(max_iter=1000)\n",
    "classifier_char.fit(X_train_clean, y_train)\n",
    "score = classifier_char.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5400457665903889"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_char.predict(X_test_clean)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['clean_tweet_without_stopwords'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['clean_tweet_without_stopwords'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,  ngram_range=(1, 3))\n",
    "vectorizer.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_clean = vectorizer.transform(tweets_train)\n",
    "X_valid_clean = vectorizer.transform(tweets_valid)\n",
    "X_test_clean  = vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "classifier_clean = LogisticRegression(max_iter=1000)\n",
    "classifier_clean.fit(X_train_clean, y_train)\n",
    "score = classifier_clean.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5300546448087432"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_clean.predict(X_test_clean)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599056603773585\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = vectorizer.transform(tweets[:9000])\n",
    "X_test_clean  = vectorizer.transform(tweets[9000:])\n",
    "y_train = y[:9000]\n",
    "y_test = y[9000:]\n",
    "\n",
    "classifier_clean = LogisticRegression(max_iter=1000)\n",
    "classifier_clean.fit(X_train_clean, y_train)\n",
    "score = classifier_clean.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed to predicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier_clean.predict(X_test_clean)\n",
    "z = zip(prediction, y_test, range(9000, 9000+len(prediction)))\n",
    "indices = np.array([i if p1!=p2 else -1 for p1, p2, i in z ])\n",
    "indices = indices[indices>0]\n",
    "for i in indices:\n",
    "    print(tweets_DF['label'][i],tweets_DF['tweet'][i], tweets_DF['negative_words'][i],  \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['embedding'].values\n",
    "tweets = np.array([np.array(t) for t in tweets]).reshape(-1, 100)\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['embedding'].values\n",
    "X_test_embedding = np.array([np.array(t) for t in tweets_test]).reshape(-1, 100)\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "X_train_embedding, X_val_embedding, y_train, y_val = train_test_split(tweets, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.326\n"
     ]
    }
   ],
   "source": [
    "classifier_embedding = LogisticRegression(max_iter=1000)\n",
    "classifier_embedding.fit(X_train_embedding, y_train)\n",
    "score = classifier_embedding.score(X_test_embedding, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_embedding.predict(X_test_embedding)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansamble(sentence):\n",
    "    \n",
    "    sentence = remove_user(sentence)\n",
    "    neg, neu, pos, comp =  sentiment_analyzer_scores(sentence)\n",
    "    negative_words = check_for_negative_words(sentence)\n",
    "    offensive_words = check_for_offensive_words(sentence)\n",
    "    \n",
    "    preproc_with_stopwords = preprocess_tweet(sentence)\n",
    "    preproc_no_stopwords = preprocess_tweet(sentence, True)\n",
    "    \n",
    "    #emb1 = embedd(preproc_with_stopwords)\n",
    "    #emb2 = embedd(preproc_no_stopwords)\n",
    "    \n",
    "    score1 = classifier_sentiment.predict([[neg, negative_words, offensive_words]])\n",
    "    \n",
    "    tfidf = vectorizer_tf.transform([sentence])\n",
    "    score2 = classifier_tfidf.predict(tfidf)\n",
    "    \n",
    "    count = vectorizer.transform([preproc_no_stopwords])\n",
    "    score3 = classifier_clean.predict(count)\n",
    "    \n",
    "    count = char_vectorizer.transform([preproc_no_stopwords])\n",
    "    score4 = classifier_char.predict(count)\n",
    "    \n",
    "    #score5 = classifier_embedding.predict([emb1])\n",
    "    \n",
    "    if not any(w in model.wv for w in preproc_no_stopwords.split()):# and score4[0]:\n",
    "            return score4[0]\n",
    "    \n",
    "    vote = collections.Counter([score1[0], score2[0], score3[0], score4[0], score3[0]])\n",
    "    if vote[0] > 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827906976744186\n",
      "0.8484848484848485\n",
      "0.4666666666666667\n",
      "0.6021505376344086\n"
     ]
    }
   ],
   "source": [
    "test_tweets = tweets_test_DF['tweet'].values\n",
    "y = tweets_test_DF['label'].values\n",
    "\n",
    "y_pred = [ansamble(t) for t in test_tweets]\n",
    "\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(precision_score(y, y_pred))\n",
    "print(recall_score(y, y_pred))\n",
    "print(f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Test sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "    label = [\"NOT\", \"OFF\"]\n",
    "    \n",
    "    sentence = remove_user(sentence)\n",
    "    neg, neu, pos, comp =  sentiment_analyzer_scores(sentence)\n",
    "    negative_words = check_for_negative_words(sentence)\n",
    "    offensive_words = check_for_offensive_words(sentence)\n",
    "    \n",
    "    preproc_with_stopwords = preprocess_tweet(sentence)\n",
    "    preproc_no_stopwords = preprocess_tweet(sentence, True)\n",
    "    \n",
    "    emb1 = embedd(preproc_with_stopwords)\n",
    "    emb2 = embedd(preproc_no_stopwords)\n",
    "    \n",
    "    score1 = classifier_sentiment.predict([[neg, negative_words, offensive_words]])\n",
    "    print(\"Features:    Classified as:\", label[score1[0]])\n",
    "    \n",
    "    tfidf = vectorizer_tf.transform([sentence])\n",
    "    score2 = classifier_tfidf.predict(tfidf)\n",
    "    print(\"TF_idf:      Classified as:\", label[score2[0]])\n",
    "    \n",
    "    count = vectorizer.transform([preproc_no_stopwords])\n",
    "    score3 = classifier_clean.predict(count)\n",
    "    print(\"Ngrams:      Classified as:\", label[score3[0]])\n",
    "    \n",
    "    count = char_vectorizer.transform([preproc_no_stopwords])\n",
    "    score4 = classifier_char.predict(count)\n",
    "    print(\"Char-ngrams: Classified as:\", label[score4[0]])\n",
    "    \n",
    "    score5 = classifier_embedding.predict([emb1])\n",
    "    print(\"Embeddings:  Classified as:\", label[score5[0]])\n",
    "    \n",
    "    score6 = ansamble(sentence)\n",
    "    print(\"Ansamble:    Classified as:\", label[score6])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"Are you fuckisiiifn in?\")\n",
    "a = \"Are you fuckisiiifn in?\"\n",
    "any([w in model.wv for w in a.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## 4. Testing correlation\n",
    "__Tested correlation of some features and labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tweets_DF['label'].values\n",
    "y_negative_words = tweets_DF['negative_words'].values\n",
    "y_offensive_words = tweets_DF['offensive_words'].values\n",
    "y_negative_sentiment = tweets_DF['negative_sentiment'].values\n",
    "y_negative_sentiment = [1 if neg > 0  else 0 for neg in y_negative_sentiment]\n",
    "\n",
    "print(\"Tweet contains negative word: \", matthews_corrcoef(y_pred, y_negative_words))\n",
    "print(\"Tweet has some negative sentiment: \", matthews_corrcoef(y_pred, y_negative_sentiment))\n",
    "print(\"Tweet contains offensive words: \", matthews_corrcoef(y_pred, y_offensive_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x = [\n",
    "    \"Negativity features\",\n",
    "    \"TF_IDF\",\n",
    "    \"char n-grams\",\n",
    "    \"word n-grams\",\n",
    "    \"ensemble\"\n",
    "]\n",
    "#x = date2num(x)\n",
    "\n",
    "olid = [0.78, 0.79, 0.76, 0.8, 0.828]\n",
    "mts = [0.705, 0.73, 0.708, 0.743, 0.752]\n",
    "\n",
    "x_pos = np.arange(len(x))\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(x_pos-0.1, olid, width=0.2, color='purple', align='center')\n",
    "ax.bar(x_pos+0.1, mts, width=0.2, color='pink', align='center')\n",
    "#ax.bar(x_pos+0.2, k, width=0.2, color='r', align='center')\n",
    "plt.xticks(x_pos, x, rotation=20)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "https://docs.google.com/document/d/1OdniS8GEYwaFJy_zNC5GpXkceuGxmOtPK_dV9QAecho/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
