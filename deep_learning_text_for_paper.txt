For deep learning approach, we've explored various architectures. As the baseline, a simple dense-only model without pretrained embeddings called BasicNoEmb has been developed. It's sole purpose is to show the difference between using pretrained embeddings and training them from scratch. 

All the other models have been trained with the help of pretrained GloVe word embeddings with the dimension of 300. In the table [x], we've shown a few of those architectures. Out of these models, the simplest one, called Basic, is exactly the same as BasicNoEmb, but it uses pretrained GloVe word embeddings and because of this performs slightly better than BasicNoEmb. Further, various combinations of Bidirectional LSTMs and CNNs are explored, in some cases used by themselves (but still combined with dense layers in the end), while in other cases LSTMs and CNNs are combined with a hope of getting better scores. 

The models shown in the table are BiLSTM which consists of an embedding layer, BiLSTM layer and three dense layers, CNN models have a 1D convolutional layer and global max pooling layer instead of BiLSTM and each one outputs a different dimension out of Conv1D, depending on the number of filters. In the table, two versions can be seen - CNN16 with 16 filters and CNN64 with 64 filters as output. Both CNN models have kernel size of 3 and a stride of 1. BiLSTM_CNN model has a BiLSTM in parallel with a CNN of kernel 3 and stride 1, whose outputs concatenated together represent the input into 3 dense layers. The most complicated model shown is BiLSTM2_CNN3. This model has four branches whose concatenated outputs go into dense layers. In the first branch, we have two BiLSTM layers on top of each other, in the second branch there is a CNN with kernel size 3, in the third branch a CNN with kernel size 4 and in the last branch there is a CNN with kernel size 5. All the CNNs have a stride of 1. The approximate architecture of this model is shown on figure [x]. 

It can be observed in the table [x] that the models that include BiLSTMs tend to perform a bit better than the ones that don't. For all the architectures, a number of different parameters and preprocessing methods were tried. Without an extensive parameter grid search, we've found empirically that having tweets preprocessed helps in the process of learning, that having less CNN filters often provides better or approximately the same score as having more CNN filters and that usually our models struggled more with recall than with precision. But overall all the models that include a BiLSTM perform very similarly. 



