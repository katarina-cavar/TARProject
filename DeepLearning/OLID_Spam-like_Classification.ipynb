{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLID Classification (structured like SMS Spam)\n",
    "\n",
    "- TextBlob: [Preventing splitting at apostrophies when tokenizing words using nltk](https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk)\n",
    "    - try `from nltk.tokenize import TweetTokenizer`\n",
    "- Argparse: [Allowing specific values for an Argparse argument [duplicate]](https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(type(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, file=\"../Dataset-OLID/OLIDv1.0/data_subtask_a.csv\"):\n",
    "        self.task_a = file\n",
    "        \n",
    "    def get_df_train_data(self):\n",
    "        train_data = pd.read_csv(self.task_a)\n",
    "        train_tweets = train_data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_df_data(self, file=\"../Dataset-OLID/OLIDv1.0/data_subtask_a.csv\"):\n",
    "        data = pd.read_csv(file)\n",
    "        train_tweets = data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_np_data_and_labels(self, file=\"../Dataset-OLID/OLIDv1.0/data_subtask_a.csv\"):\n",
    "        tweets = self.get_df_data(file)\n",
    "        data, labels = tweets.values[:,0], tweets.values[:,1]\n",
    "        return data, labels\n",
    "    \n",
    "    # this creates copies\n",
    "    def shuffle_np(self, data, labels):\n",
    "        assert len(data) == len(labels)\n",
    "        p = np.random.permutation(len(data))\n",
    "        return data[p], labels[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'str'>\n",
      "@USER She should ask a few native Americans what their take on this is.\n",
      "--------------------------------------------------\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader()\n",
    "data, labels = dr.get_np_data_and_labels()\n",
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "print(data[0])\n",
    "print(\"-\"*50)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No point in doing this, these words are already in :)\n",
    "\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#import re\n",
    "\n",
    "#stop_custom = stopwords.words(\"english\")\n",
    "\n",
    "#print(stop_custom)\n",
    "\n",
    "#stop_custom.append(\".\")\n",
    "#stop_custom.append(\"!\")\n",
    "#stop_custom.append(\"'\")\n",
    "\n",
    "#for stopword in stop_custom:\n",
    "#    if \"'\" in stopword:\n",
    "#        print(stopword)\n",
    "#        pre, post = stopword.split(\"'\")\n",
    "#        \n",
    "#        print(pre, post)\n",
    "#        if pre not in stop_custom: \n",
    "#            stop_custom.append(pre)\n",
    "#            print(\"ADDED: \", pre)\n",
    "#        if post not in stop_custom: \n",
    "#           stop_custom.append(post)\n",
    "#            print(\"ADDED: \", post)\n",
    "\n",
    "#print(stop_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def no_preprocessing(self, data, verbose=False):\n",
    "        return data\n",
    "\n",
    "    def remove_punctuation(self, data, verbose=False):\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "\n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            data[i] = sentence\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"â€™\")\n",
    "        stop.append(\"@user\")\n",
    "        \n",
    "        tknzr = TweetTokenizer()\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        #noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            #sentence_blob = TextBlob(data[i])\n",
    "            sentence_blob = tknzr.tokenize(data[i])\n",
    "            #print(\"Blob: \", sentence_blob)\n",
    "            sentence = \" \".join(sentence_blob) #.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            #words = data[i].split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation_textblob(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"â€™\")\n",
    "        stop.append(\"user\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        #noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['ask native americans take .'\n",
      " 'go home drunk ! ! ! #maga #trump2020 ðŸ‘Š ðŸ‡º ðŸ‡¸ ðŸ‘Š url'\n",
      " 'amazon investigating chinese employees selling internal data third-party sellers looking edge competitive marketplace . url #amazon #maga #kag #china #tcot'\n",
      " 'someone should\\'vetaken \" piece shit volcano . ðŸ˜‚ \"'\n",
      " 'obama wanted liberals & illegals move red states'\n",
      " 'liberals kookoo ! ! !' 'oh noes ! tough shit .'\n",
      " 'literally talking lol mass shootings like set ups . propaganda used divide us major issues like gun control terrorism'\n",
      " 'buy icecream ! ! !'\n",
      " 'canada need another cuck ! already enough #looneyleft #liberals f * * king great country ! #qproofs #trudeaumustgo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_stopwords_and_punctuation(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['ask native americans take' 'go home drunk maga trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š url'\n",
      " 'amazon investigating chinese employees selling internal data third-party sellers looking edge competitive marketplace url amazon maga kag china tcot'\n",
      " \"someone should'vetaken piece shit volcano ðŸ˜‚\"\n",
      " 'obama wanted liberals amp illegals move red states' 'liberals kookoo'\n",
      " 'oh noes tough shit'\n",
      " 'literally talking lol mass shootings like set ups propaganda used divide us major issues like gun control terrorism'\n",
      " 'buy icecream'\n",
      " 'canada need another cuck already enough looneyleft liberals f king great country qproofs trudeaumustgo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_stopwords_and_punctuation_textblob(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.no_preprocessing(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['USER She should ask a few native Americans what their take on this is'\n",
      " 'USER USER Go home you â€™ re drunk USER MAGA Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace URL Amazon MAGA KAG CHINA TCOT'\n",
      " \"USER Someone should'veTaken this piece of shit to a volcano ðŸ˜‚\"\n",
      " 'USER USER Obama wanted liberals amp illegals to move into red states'\n",
      " 'USER Liberals are all Kookoo' 'USER USER Oh noes Tough shit'\n",
      " 'USER was literally just talking about this lol all mass shootings like that have been set ups it â€™ s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " 'USER Buy more icecream'\n",
      " 'USER Canada doesn â€™ t need another CUCK We already have enough LooneyLeft Liberals f king up our great country Qproofs TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_punctuation(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Preprocessor.no_preprocessing of <__main__.Preprocessor object at 0x000001D35D491208>>\n",
      "<class 'numpy.ndarray'>\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "a=pp.no_preprocessing\n",
    "print(a)\n",
    "data_no_stopwords = a(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess actual whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = pp.remove_stopwords_and_punctuation_textblob(data.copy(),\n",
    "                                                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "13240 13240\n",
      "['ask native americans take' 'go home drunk maga trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š url'\n",
      " 'amazon investigating chinese employees selling internal data third-party sellers looking edge competitive marketplace url amazon maga kag china tcot'\n",
      " \"someone should'vetaken piece shit volcano ðŸ˜‚\"\n",
      " 'obama wanted liberals amp illegals move red states' 'liberals kookoo'\n",
      " 'oh noes tough shit'\n",
      " 'literally talking lol mass shootings like set ups propaganda used divide us major issues like gun control terrorism'\n",
      " 'buy icecream'\n",
      " 'canada need another cuck already enough looneyleft liberals f king great country qproofs trudeaumustgo']\n",
      "[1 1 0 1 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(clean_tweets), type(labels))\n",
    "print(len(clean_tweets), len(labels))\n",
    "print(clean_tweets[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(msgs) = 13240; len(labels) = 13240\n",
      "TRAIN: len(x) = 10592; len(y) = 10592\n",
      "TEST: len(x) = 2648; len(y) = 2648\n",
      "\n",
      "len(word_index) = 17739\n",
      "\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VOCAB_SIZE = 2500\n",
    "EMBEDDING_DIM = 16\n",
    "MAX_LENGTH = 64\n",
    "TRUNC_TYPE = \"post\"\n",
    "PADDING_TYPE = \"post\"\n",
    "OOV_TOK = \"<OOV>\"\n",
    "TRAINING_PORTION = 0.8\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "def get_train_val(messages, labels):\n",
    "    train_number = int(len(messages) * TRAINING_PORTION)\n",
    "\n",
    "    train_msgs = messages[:train_number]\n",
    "    train_labels = labels[:train_number]\n",
    "    val_msgs = messages[train_number:]\n",
    "    val_labels = labels[train_number:]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token = OOV_TOK)\n",
    "    tokenizer.fit_on_texts(train_msgs)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\"len(msgs) = {}; len(labels) = {}\".format(len(messages), len(labels)))\n",
    "    print(\"TRAIN: len(x) = {}; len(y) = {}\".format(len(train_msgs),len(train_labels)))\n",
    "    print(\"TEST: len(x) = {}; len(y) = {}\".format(len(val_msgs),len(val_labels)))\n",
    "\n",
    "\n",
    "    print(\"\\nlen(word_index) = {}\\n\".format(len(word_index))) \n",
    "    # Total number of words without stopwords = 8029\n",
    "    #print(word_index)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_msgs)\n",
    "    train_padded = pad_sequences(train_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    val_sequences = tokenizer.texts_to_sequences(val_msgs)\n",
    "    val_padded = pad_sequences(val_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    print(type(val_padded))\n",
    "\n",
    "    #label_tokenizer = Tokenizer()\n",
    "    #label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "    #train_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "    #val_label_seq = np.array(label_tokenizer.texts_to_sequences(val_labels))\n",
    "    \n",
    "    return train_padded, train_labels, val_padded, val_labels\n",
    "\n",
    "\n",
    "\n",
    "#    \"\"\"\n",
    "#        Set labels to be 0 for ham, 1 for spam\n",
    "#        (was 1 for ham, 2 for spam before -> didn't work)\n",
    "#    \"\"\"\n",
    "#    for i in range(len(train_label_seq)):\n",
    "#        train_label_seq[i] -= 1\n",
    "#\n",
    "#    for i in range(len(val_label_seq)):\n",
    "#        val_label_seq[i] -= 1\n",
    "\n",
    "\n",
    "train_x, train_y, val_x, val_y = get_train_val(clean_tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: len(x) = 10592; len(y) = 10592\n",
      "VAL: len(x) = 2648; len(y) = 2648\n",
      "\n",
      "[ 249 2116  186   49    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0] -> len =  64\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN: len(x) = {}; len(y) = {}\".format(len(train_x),len(train_y)))\n",
    "print(\"VAL: len(x) = {}; len(y) = {}\".format(len(val_x),len(val_y)))\n",
    "print()\n",
    "\n",
    "print(train_x[0], \"-> len = \", len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 64, 16)            40000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 40,433\n",
      "Trainable params: 40,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[ 249 2116  186 ...    0    0    0]\n",
      " [  24  207  816 ...    0    0    0]\n",
      " [   1    1 2117 ...    0    0    0]\n",
      " ...\n",
      " [ 145    8   41 ...    0    0    0]\n",
      " [   1   17    1 ...    0    0    0]\n",
      " [1504 1417    0 ...    0    0    0]]\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 1s 66us/sample - loss: 0.6384 - acc: 0.6671 - val_loss: 0.6303 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.6268 - acc: 0.6674 - val_loss: 0.6223 - val_acc: 0.6688\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.6070 - acc: 0.6725 - val_loss: 0.5898 - val_acc: 0.6832\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 0s 47us/sample - loss: 0.5422 - acc: 0.7294 - val_loss: 0.5436 - val_acc: 0.7406\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 0s 45us/sample - loss: 0.4794 - acc: 0.7794 - val_loss: 0.5369 - val_acc: 0.7583\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 1s 47us/sample - loss: 0.4430 - acc: 0.7995 - val_loss: 0.5296 - val_acc: 0.7594\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 1s 50us/sample - loss: 0.4187 - acc: 0.8157 - val_loss: 0.5462 - val_acc: 0.7591\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.4042 - acc: 0.8226 - val_loss: 0.5450 - val_acc: 0.7636\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 1s 50us/sample - loss: 0.3931 - acc: 0.8288 - val_loss: 0.5533 - val_acc: 0.7662\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 1s 50us/sample - loss: 0.3841 - acc: 0.8341 - val_loss: 0.5629 - val_acc: 0.7632\n",
      "Epoch 11/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.3775 - acc: 0.8381 - val_loss: 0.5729 - val_acc: 0.7583\n",
      "Epoch 12/20\n",
      "10592/10592 [==============================] - 1s 51us/sample - loss: 0.3701 - acc: 0.8412 - val_loss: 0.5819 - val_acc: 0.7625\n",
      "Epoch 13/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.3675 - acc: 0.8444 - val_loss: 0.5908 - val_acc: 0.7636\n",
      "Epoch 14/20\n",
      "10592/10592 [==============================] - 1s 50us/sample - loss: 0.3633 - acc: 0.8457 - val_loss: 0.5999 - val_acc: 0.7606\n",
      "Epoch 15/20\n",
      "10592/10592 [==============================] - 1s 51us/sample - loss: 0.3590 - acc: 0.8454 - val_loss: 0.6142 - val_acc: 0.7470\n",
      "Epoch 16/20\n",
      "10592/10592 [==============================] - 1s 48us/sample - loss: 0.3560 - acc: 0.8486 - val_loss: 0.6247 - val_acc: 0.7432\n",
      "Epoch 17/20\n",
      "10592/10592 [==============================] - 1s 47us/sample - loss: 0.3544 - acc: 0.8501 - val_loss: 0.6216 - val_acc: 0.7481\n",
      "Epoch 18/20\n",
      "10592/10592 [==============================] - 1s 47us/sample - loss: 0.3499 - acc: 0.8523 - val_loss: 0.6310 - val_acc: 0.7508\n",
      "Epoch 19/20\n",
      "10592/10592 [==============================] - 1s 47us/sample - loss: 0.3470 - acc: 0.8526 - val_loss: 0.6439 - val_acc: 0.7440\n",
      "Epoch 20/20\n",
      "10592/10592 [==============================] - 1s 47us/sample - loss: 0.3467 - acc: 0.8533 - val_loss: 0.6430 - val_acc: 0.7496\n",
      "2648/2648 [==============================] - 0s 19us/sample - loss: 0.6430 - acc: 0.7496\n",
      "\n",
      "SCORES:\n",
      "[0.6430439380900738, 0.74962234]\n",
      "\n",
      "--> Saved model to disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4d2f8892ff91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mreverse_word_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Define the model\n",
    "\"\"\"\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "print(train_x)\n",
    "print(type(train_x))\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "         epochs=NUM_EPOCHS, validation_data=(val_x, val_y))\n",
    "\n",
    "scores = model.evaluate(val_x, val_y)\n",
    "\n",
    "print(\"\\nSCORES:\")\n",
    "print(scores)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Save model to json\n",
    "\"\"\"\n",
    "\n",
    "# serialize model to json\n",
    "model_json = model.to_json()\n",
    "with open(\"model_sms_spam.json\", \"w\") as json_file: \n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_sms_spam.h5\")\n",
    "print(\"\\n--> Saved model to disk.\")\n",
    "\n",
    "#\"\"\"\n",
    "#    reset labels to be 1 for ham, 2 for spam\n",
    "#    (was 1 for ham, 2 for spam before -> didn't work)\n",
    "#\"\"\"\n",
    "#for i in range(len(train_label_seq)):\n",
    "#    train_label_seq[i] += 1\n",
    "\n",
    "#for i in range(len(val_label_seq)):\n",
    "#    val_label_seq[i] += 1\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "  \n",
    "plot_graphs(history, \"acc\")\n",
    "plot_graphs(history, \"loss\")\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_sentence(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
