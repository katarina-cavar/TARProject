{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLID Classification (structured like SMS Spam)\n",
    "\n",
    "- TextBlob: [Preventing splitting at apostrophies when tokenizing words using nltk](https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk)\n",
    "    - try `from nltk.tokenize import TweetTokenizer`\n",
    "- Argparse: [Allowing specific values for an Argparse argument [duplicate]](https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(type(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, file=\"../Dataset-OLID/OLIDv1.0/data_subtask_a.csv\"):\n",
    "        self.task_a = file\n",
    "        \n",
    "    def get_df_train_data(self):\n",
    "        train_data = pd.read_csv(self.task_a)\n",
    "        train_tweets = train_data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_df_data(self, file=\"data_subtask_a.csv\"):\n",
    "        data = pd.read_csv(file)\n",
    "        train_tweets = data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_np_data_and_labels(self, file=\"data_subtask_a.csv\"):\n",
    "        tweets = self.get_df_data(file)\n",
    "        data, labels = tweets.values[:,0], tweets.values[:,1]\n",
    "        return data, labels\n",
    "    \n",
    "    # this creates copies\n",
    "    def shuffle_np(self, data, labels):\n",
    "        assert len(data) == len(labels)\n",
    "        p = np.random.permutation(len(data))\n",
    "        return data[p], labels[p]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'str'>\n",
      "@USER She should ask a few native Americans what their take on this is.\n",
      "--------------------------------------------------\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader()\n",
    "data, labels = dr.get_np_data_and_labels()\n",
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "print(data[0])\n",
    "print(\"-\"*50)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No point in doing this, these words are already in :)\n",
    "\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#import re\n",
    "\n",
    "#stop_custom = stopwords.words(\"english\")\n",
    "\n",
    "#print(stop_custom)\n",
    "\n",
    "#stop_custom.append(\".\")\n",
    "#stop_custom.append(\"!\")\n",
    "#stop_custom.append(\"'\")\n",
    "\n",
    "#for stopword in stop_custom:\n",
    "#    if \"'\" in stopword:\n",
    "#        print(stopword)\n",
    "#        pre, post = stopword.split(\"'\")\n",
    "#        \n",
    "#        print(pre, post)\n",
    "#        if pre not in stop_custom: \n",
    "#            stop_custom.append(pre)\n",
    "#            print(\"ADDED: \", pre)\n",
    "#        if post not in stop_custom: \n",
    "#           stop_custom.append(post)\n",
    "#            print(\"ADDED: \", post)\n",
    "\n",
    "#print(stop_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def no_preprocessing(self, data, verbose=False):\n",
    "        return data\n",
    "\n",
    "    def remove_punctuation(self, data, verbose=False):\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "\n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            data[i] = sentence\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"â€™\")\n",
    "        stop.append(\"@user\")\n",
    "        \n",
    "        tknzr = TweetTokenizer()\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        #noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            #sentence_blob = TextBlob(data[i])\n",
    "            sentence_blob = tknzr.tokenize(data[i])\n",
    "            #print(\"Blob: \", sentence_blob)\n",
    "            sentence = \" \".join(sentence_blob) #.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            #words = data[i].split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation_textblob(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"â€™\")\n",
    "        stop.append(\"user\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        #noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n",
    "    \n",
    "    def remove_all_but_words(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"â€™\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['@user ask native americans take .'\n",
      " '@user @user go home drunk ! ! ! @user #maga #trump2020 ðŸ‘Š ðŸ‡º ðŸ‡¸ ðŸ‘Š url'\n",
      " 'amazon investigating chinese employees selling internal data third-party sellers looking edge competitive marketplace . url #amazon #maga #kag #china #tcot'\n",
      " '@user someone should\\'vetaken \" piece shit volcano . ðŸ˜‚ \"'\n",
      " '@user @user obama wanted liberals & illegals move red states'\n",
      " '@user liberals kookoo ! ! !' '@user @user oh noes ! tough shit .'\n",
      " '@user literally talking lol mass shootings like set ups . propaganda used divide us major issues like gun control terrorism'\n",
      " '@user buy icecream ! ! !'\n",
      " '@user canada need another cuck ! already enough #looneyleft #liberals f * * king great country ! #qproofs #trudeaumustgo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_stopwords_and_punctuation(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['user ask native americans take'\n",
      " 'user user go home drunk user maga trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š url'\n",
      " 'amazon investigating chinese employees selling internal data third-party sellers looking edge competitive marketplace url amazon maga kag china tcot'\n",
      " \"user someone should'vetaken piece shit volcano ðŸ˜‚\"\n",
      " 'user user obama wanted liberals amp illegals move red states'\n",
      " 'user liberals kookoo' 'user user oh noes tough shit'\n",
      " 'user literally talking lol mass shootings like set ups propaganda used divide us major issues like gun control terrorism'\n",
      " 'user buy icecream'\n",
      " 'user canada need another cuck already enough looneyleft liberals f king great country qproofs trudeaumustgo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_stopwords_and_punctuation_textblob(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.no_preprocessing(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['USER She should ask a few native Americans what their take on this is'\n",
      " 'USER USER Go home you â€™ re drunk USER MAGA Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace URL Amazon MAGA KAG CHINA TCOT'\n",
      " \"USER Someone should'veTaken this piece of shit to a volcano ðŸ˜‚\"\n",
      " 'USER USER Obama wanted liberals amp illegals to move into red states'\n",
      " 'USER Liberals are all Kookoo' 'USER USER Oh noes Tough shit'\n",
      " 'USER was literally just talking about this lol all mass shootings like that have been set ups it â€™ s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " 'USER Buy more icecream'\n",
      " 'USER Canada doesn â€™ t need another CUCK We already have enough LooneyLeft Liberals f king up our great country Qproofs TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "data_no_stopwords = pp.remove_punctuation(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Preprocessor.no_preprocessing of <__main__.Preprocessor object at 0x000001A3D8E5DB08>>\n",
      "<class 'numpy.ndarray'>\n",
      "['@USER She should ask a few native Americans what their take on this is.'\n",
      " '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL'\n",
      " 'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT'\n",
      " '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"'\n",
      " '@USER @USER Obama wanted liberals &amp; illegals to move into red states'\n",
      " '@USER Liberals are all Kookoo !!!' '@USER @USER Oh noes! Tough shit.'\n",
      " '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism'\n",
      " '@USER Buy more icecream!!!'\n",
      " '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo']\n"
     ]
    }
   ],
   "source": [
    "a=pp.no_preprocessing\n",
    "print(a)\n",
    "data_no_stopwords = a(data[:10].copy(), \n",
    "                                                        verbose=False)\n",
    "print(type(data_no_stopwords))\n",
    "print(data_no_stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess actual whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = pp.remove_stopwords_and_punctuation_textblob(data.copy(),\n",
    "                                                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(clean_tweets), type(labels))\n",
    "print(len(clean_tweets), len(labels))\n",
    "print(clean_tweets[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VOCAB_SIZE = 2500\n",
    "EMBEDDING_DIM = 16\n",
    "MAX_LENGTH = 64\n",
    "TRUNC_TYPE = \"post\"\n",
    "PADDING_TYPE = \"post\"\n",
    "OOV_TOK = \"<OOV>\"\n",
    "TRAINING_PORTION = 0.8\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "def get_train_val(messages, labels):\n",
    "    train_number = int(len(messages) * TRAINING_PORTION)\n",
    "\n",
    "    train_msgs = messages[:train_number]\n",
    "    train_labels = labels[:train_number]\n",
    "    val_msgs = messages[train_number:]\n",
    "    val_labels = labels[train_number:]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token = OOV_TOK)\n",
    "    tokenizer.fit_on_texts(train_msgs)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\"len(msgs) = {}; len(labels) = {}\".format(len(messages), len(labels)))\n",
    "    print(\"TRAIN: len(x) = {}; len(y) = {}\".format(len(train_msgs),len(train_labels)))\n",
    "    print(\"TEST: len(x) = {}; len(y) = {}\".format(len(val_msgs),len(val_labels)))\n",
    "\n",
    "\n",
    "    print(\"\\nlen(word_index) = {}\\n\".format(len(word_index))) \n",
    "    # Total number of words without stopwords = 8029\n",
    "    #print(word_index)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_msgs)\n",
    "    train_padded = pad_sequences(train_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    val_sequences = tokenizer.texts_to_sequences(val_msgs)\n",
    "    val_padded = pad_sequences(val_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    print(type(val_padded))\n",
    "\n",
    "    #label_tokenizer = Tokenizer()\n",
    "    #label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "    #train_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "    #val_label_seq = np.array(label_tokenizer.texts_to_sequences(val_labels))\n",
    "    \n",
    "    return train_padded, train_labels, val_padded, val_labels\n",
    "\n",
    "\n",
    "\n",
    "#    \"\"\"\n",
    "#        Set labels to be 0 for ham, 1 for spam\n",
    "#        (was 1 for ham, 2 for spam before -> didn't work)\n",
    "#    \"\"\"\n",
    "#    for i in range(len(train_label_seq)):\n",
    "#        train_label_seq[i] -= 1\n",
    "#\n",
    "#    for i in range(len(val_label_seq)):\n",
    "#        val_label_seq[i] -= 1\n",
    "\n",
    "\n",
    "train_x, train_y, val_x, val_y = get_train_val(clean_tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN: len(x) = {}; len(y) = {}\".format(len(train_x),len(train_y)))\n",
    "print(\"VAL: len(x) = {}; len(y) = {}\".format(len(val_x),len(val_y)))\n",
    "print()\n",
    "\n",
    "print(train_x[0], \"-> len = \", len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Define the model\n",
    "\"\"\"\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "print(train_x)\n",
    "print(type(train_x))\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "         epochs=NUM_EPOCHS, validation_data=(val_x, val_y))\n",
    "\n",
    "scores = model.evaluate(val_x, val_y)\n",
    "\n",
    "print(\"\\nSCORES:\")\n",
    "print(scores)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Save model to json\n",
    "\"\"\"\n",
    "\n",
    "# serialize model to json\n",
    "model_json = model.to_json()\n",
    "with open(\"model_sms_spam.json\", \"w\") as json_file: \n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_sms_spam.h5\")\n",
    "print(\"\\n--> Saved model to disk.\")\n",
    "\n",
    "#\"\"\"\n",
    "#    reset labels to be 1 for ham, 2 for spam\n",
    "#    (was 1 for ham, 2 for spam before -> didn't work)\n",
    "#\"\"\"\n",
    "#for i in range(len(train_label_seq)):\n",
    "#    train_label_seq[i] += 1\n",
    "\n",
    "#for i in range(len(val_label_seq)):\n",
    "#    val_label_seq[i] += 1\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "  \n",
    "plot_graphs(history, \"acc\")\n",
    "plot_graphs(history, \"loss\")\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_sentence(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
