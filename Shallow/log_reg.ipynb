{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- __Model info:__\n",
    "    - features:\n",
    "        - contains negative words (T/F)\n",
    "        - contains offensive words (T/F)\n",
    "        - sentiment (+, -, 0)\n",
    "        - contains emoji\n",
    "    - model: Logistic Regression\n",
    "    - max acc: 80% counter with ngrams(1, 3)\n",
    "    \n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(['stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              tweet  \\\n",
       "0           0  86426  @USER She should ask a few native Americans wh...   \n",
       "1           1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...   \n",
       "2           2  16820  Amazon is investigating Chinese employees who ...   \n",
       "3           3  62688  @USER Someone should'veTaken\" this piece of sh...   \n",
       "4           4  43605  @USER @USER Obama wanted liberals &amp; illega...   \n",
       "5           5  97670                  @USER Liberals are all Kookoo !!!   \n",
       "6           6  77444                   @USER @USER Oh noes! Tough shit.   \n",
       "7           7  52415  @USER was literally just talking about this lo...   \n",
       "8           8  45157                         @USER Buy more icecream!!!   \n",
       "9           9  13384  @USER Canada doesn’t need another CUCK! We alr...   \n",
       "\n",
       "  subtask_a  label_a  \n",
       "0       OFF        1  \n",
       "1       OFF        1  \n",
       "2       NOT        0  \n",
       "3       OFF        1  \n",
       "4       NOT        0  \n",
       "5       OFF        1  \n",
       "6       OFF        1  \n",
       "7       OFF        1  \n",
       "8       NOT        0  \n",
       "9       OFF        1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_DF = pd.read_csv('../Dataset-OLID/OLIDv1.0/data_subtask_a.csv')\n",
    "tweets_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_DF = pd.read_csv('../Dataset-OLID/OLIDv1.0/test_data_subtask_a.csv')\n",
    "tweets_test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_DF = pd.read_csv('../Dataset-TSA/sentiment500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>id</th>\n",
       "      <th>label_a</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>58314</td>\n",
       "      <td>58314</td>\n",
       "      <td>58314</td>\n",
       "      <td>58326</td>\n",
       "      <td>0</td>\n",
       "      <td>@Beliria @vinworldwide hello friends!  Hope yo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>21962</td>\n",
       "      <td>21962</td>\n",
       "      <td>21962</td>\n",
       "      <td>21974</td>\n",
       "      <td>1</td>\n",
       "      <td>@3drik n im tryna go hoop.....wish i had a boy...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>57878</td>\n",
       "      <td>57878</td>\n",
       "      <td>57878</td>\n",
       "      <td>57890</td>\n",
       "      <td>0</td>\n",
       "      <td>@beehughes118 Thank you for the follow.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>58146</td>\n",
       "      <td>58146</td>\n",
       "      <td>58146</td>\n",
       "      <td>58158</td>\n",
       "      <td>0</td>\n",
       "      <td>@beatmanludmilla Thats amaizing...Thanks a lot...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>57954</td>\n",
       "      <td>57954</td>\n",
       "      <td>57954</td>\n",
       "      <td>57966</td>\n",
       "      <td>0</td>\n",
       "      <td>@beamadelica ha! great stuff!  i use a few of ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>58332</td>\n",
       "      <td>58332</td>\n",
       "      <td>58332</td>\n",
       "      <td>58344</td>\n",
       "      <td>0</td>\n",
       "      <td>@beauty411 ohhhhh!  that explains it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>74899</td>\n",
       "      <td>74899</td>\n",
       "      <td>74899</td>\n",
       "      <td>74911</td>\n",
       "      <td>1</td>\n",
       "      <td>@BryanRicard im football illiterate</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>58233</td>\n",
       "      <td>58233</td>\n",
       "      <td>58233</td>\n",
       "      <td>58245</td>\n",
       "      <td>0</td>\n",
       "      <td>@Bel @Moldor refused to let them cut his jacke...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>22112</td>\n",
       "      <td>22112</td>\n",
       "      <td>22112</td>\n",
       "      <td>22124</td>\n",
       "      <td>1</td>\n",
       "      <td>@416Jamz you replaced me</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>58080</td>\n",
       "      <td>58080</td>\n",
       "      <td>58080</td>\n",
       "      <td>58092</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeastTheSkitzo l0l niggaaaa i dont act up i j...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>58304</td>\n",
       "      <td>58304</td>\n",
       "      <td>58304</td>\n",
       "      <td>58316</td>\n",
       "      <td>0</td>\n",
       "      <td>@belindaang haha. then coffee was a good choice</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>62720</td>\n",
       "      <td>62720</td>\n",
       "      <td>62720</td>\n",
       "      <td>62732</td>\n",
       "      <td>1</td>\n",
       "      <td>@bikermom69  Get my email - hee hee!</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>58286</td>\n",
       "      <td>58286</td>\n",
       "      <td>58286</td>\n",
       "      <td>58298</td>\n",
       "      <td>0</td>\n",
       "      <td>@BelgianWaffling sniffing Neurofen plus? pray ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>21955</td>\n",
       "      <td>21955</td>\n",
       "      <td>21955</td>\n",
       "      <td>21967</td>\n",
       "      <td>1</td>\n",
       "      <td>@3btracks   I'm not performing anymore, last m...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>62889</td>\n",
       "      <td>62889</td>\n",
       "      <td>62889</td>\n",
       "      <td>62901</td>\n",
       "      <td>1</td>\n",
       "      <td>@billpalmer welcome to the new facebook!</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>58357</td>\n",
       "      <td>58357</td>\n",
       "      <td>58357</td>\n",
       "      <td>58369</td>\n",
       "      <td>0</td>\n",
       "      <td>@beautyworks Get 100 followers a day using www...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>62413</td>\n",
       "      <td>62413</td>\n",
       "      <td>62413</td>\n",
       "      <td>62425</td>\n",
       "      <td>1</td>\n",
       "      <td>@BigTastyBurger Yeah I noticed. DonkeyBiscuits...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>58003</td>\n",
       "      <td>58003</td>\n",
       "      <td>58003</td>\n",
       "      <td>58015</td>\n",
       "      <td>0</td>\n",
       "      <td>@beardedbrain http://twitpic.com/50pas - u my ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>22014</td>\n",
       "      <td>22014</td>\n",
       "      <td>22014</td>\n",
       "      <td>22026</td>\n",
       "      <td>1</td>\n",
       "      <td>@_kahlaaa Yeah it is hahaha the picture is rea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>22009</td>\n",
       "      <td>1</td>\n",
       "      <td>@_Jodi *hugs* sorry you didn't win</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>62914</td>\n",
       "      <td>62914</td>\n",
       "      <td>62914</td>\n",
       "      <td>62926</td>\n",
       "      <td>1</td>\n",
       "      <td>@billy_norris I bet you guys sound awesome! Wi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>62777</td>\n",
       "      <td>62777</td>\n",
       "      <td>62777</td>\n",
       "      <td>62789</td>\n",
       "      <td>1</td>\n",
       "      <td>@blinkandyoumiss sigh. he's mean</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>58334</td>\n",
       "      <td>58334</td>\n",
       "      <td>58334</td>\n",
       "      <td>58346</td>\n",
       "      <td>0</td>\n",
       "      <td>@beautybanter PLEASE HELP PASS THIS ON FOR THE...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>62644</td>\n",
       "      <td>62644</td>\n",
       "      <td>62644</td>\n",
       "      <td>62656</td>\n",
       "      <td>1</td>\n",
       "      <td>@blazingviolet ppps can u apologises to Fat Bu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>21951</td>\n",
       "      <td>21951</td>\n",
       "      <td>21951</td>\n",
       "      <td>21963</td>\n",
       "      <td>1</td>\n",
       "      <td>@3aneeda me too want lunch  9a7teen</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>21947</td>\n",
       "      <td>21947</td>\n",
       "      <td>21947</td>\n",
       "      <td>21959</td>\n",
       "      <td>1</td>\n",
       "      <td>@_jaye i'm on the fence. it's more like a help...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>62880</td>\n",
       "      <td>62880</td>\n",
       "      <td>62880</td>\n",
       "      <td>62892</td>\n",
       "      <td>1</td>\n",
       "      <td>@billkaulitzlovr It was just a horrible day.  ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>58246</td>\n",
       "      <td>58246</td>\n",
       "      <td>58246</td>\n",
       "      <td>58258</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeauGiles You're providing me entertainment, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>57998</td>\n",
       "      <td>57998</td>\n",
       "      <td>57998</td>\n",
       "      <td>58010</td>\n",
       "      <td>0</td>\n",
       "      <td>@bear2care2009 Don't stick your head out of tr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>58123</td>\n",
       "      <td>58123</td>\n",
       "      <td>58123</td>\n",
       "      <td>58135</td>\n",
       "      <td>0</td>\n",
       "      <td>@begthehobo LOL, only 3?!  Ive been going insa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>58369</td>\n",
       "      <td>58369</td>\n",
       "      <td>58369</td>\n",
       "      <td>58381</td>\n",
       "      <td>0</td>\n",
       "      <td>@bella456 G'morning! Have an awesome Aloha Fri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>62782</td>\n",
       "      <td>62782</td>\n",
       "      <td>62782</td>\n",
       "      <td>62794</td>\n",
       "      <td>1</td>\n",
       "      <td>@blip The 30 sec extracts are boring  I want t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>58110</td>\n",
       "      <td>58110</td>\n",
       "      <td>58110</td>\n",
       "      <td>58122</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeffyJo You have a list of suspects?! I shoul...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>58158</td>\n",
       "      <td>58158</td>\n",
       "      <td>58158</td>\n",
       "      <td>58170</td>\n",
       "      <td>0</td>\n",
       "      <td>@Beattifickid89 I can't get a break today, can...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>62445</td>\n",
       "      <td>62445</td>\n",
       "      <td>62445</td>\n",
       "      <td>62457</td>\n",
       "      <td>1</td>\n",
       "      <td>@biilly &amp;quot;Sorry, currently our video libra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>22223</td>\n",
       "      <td>22223</td>\n",
       "      <td>22223</td>\n",
       "      <td>22235</td>\n",
       "      <td>1</td>\n",
       "      <td>@_meeks_ Omg don't scare me like that</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>57953</td>\n",
       "      <td>57953</td>\n",
       "      <td>57953</td>\n",
       "      <td>57965</td>\n",
       "      <td>0</td>\n",
       "      <td>@beam_orangery few = phew...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>62447</td>\n",
       "      <td>62447</td>\n",
       "      <td>62447</td>\n",
       "      <td>62459</td>\n",
       "      <td>1</td>\n",
       "      <td>@BlackMamba23 you included people you don't ev...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>62741</td>\n",
       "      <td>62741</td>\n",
       "      <td>62741</td>\n",
       "      <td>62753</td>\n",
       "      <td>1</td>\n",
       "      <td>@Bill_Cameron I know, just trying to look on t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>22101</td>\n",
       "      <td>22101</td>\n",
       "      <td>22101</td>\n",
       "      <td>22113</td>\n",
       "      <td>1</td>\n",
       "      <td>@3zlyca Hope your back feels better today.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>62674</td>\n",
       "      <td>62674</td>\n",
       "      <td>62674</td>\n",
       "      <td>62686</td>\n",
       "      <td>1</td>\n",
       "      <td>@bleedingxsoul He said that he thought I was t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>57995</td>\n",
       "      <td>57995</td>\n",
       "      <td>57995</td>\n",
       "      <td>58007</td>\n",
       "      <td>0</td>\n",
       "      <td>@bear_eyes ) Are we still up for tomorrow?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>62811</td>\n",
       "      <td>62811</td>\n",
       "      <td>62811</td>\n",
       "      <td>62823</td>\n",
       "      <td>1</td>\n",
       "      <td>@blkwithwhtstrpz lmao oh yea! Hmmm...this celi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>58165</td>\n",
       "      <td>58165</td>\n",
       "      <td>58165</td>\n",
       "      <td>58177</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeaucoupKevin what is a decent blog software/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>22021</td>\n",
       "      <td>22021</td>\n",
       "      <td>22021</td>\n",
       "      <td>22033</td>\n",
       "      <td>1</td>\n",
       "      <td>@3LL3N it was the best day! idunno where it is...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>62649</td>\n",
       "      <td>62649</td>\n",
       "      <td>62649</td>\n",
       "      <td>62661</td>\n",
       "      <td>1</td>\n",
       "      <td>@Blazulka96 This is interesting  I am bored</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>58344</td>\n",
       "      <td>58344</td>\n",
       "      <td>58344</td>\n",
       "      <td>58356</td>\n",
       "      <td>0</td>\n",
       "      <td>@beautyfulashley</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>62808</td>\n",
       "      <td>62808</td>\n",
       "      <td>62808</td>\n",
       "      <td>62820</td>\n",
       "      <td>1</td>\n",
       "      <td>@BLKPARISHILTON1 mine either</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>62909</td>\n",
       "      <td>62909</td>\n",
       "      <td>62909</td>\n",
       "      <td>62921</td>\n",
       "      <td>1</td>\n",
       "      <td>@Billy I stayed up all night listening to &amp;quo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>62847</td>\n",
       "      <td>62847</td>\n",
       "      <td>62847</td>\n",
       "      <td>62859</td>\n",
       "      <td>1</td>\n",
       "      <td>@BillBubbaBussey Rick is still old school text...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>62661</td>\n",
       "      <td>62661</td>\n",
       "      <td>62661</td>\n",
       "      <td>62673</td>\n",
       "      <td>1</td>\n",
       "      <td>@BleacherBabble</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>22224</td>\n",
       "      <td>22224</td>\n",
       "      <td>22224</td>\n",
       "      <td>22236</td>\n",
       "      <td>1</td>\n",
       "      <td>@_meeks_ OMG! i think something is wrong with ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>58232</td>\n",
       "      <td>58232</td>\n",
       "      <td>58232</td>\n",
       "      <td>58244</td>\n",
       "      <td>0</td>\n",
       "      <td>@BeKultured I try my best. Sometimes with succ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>58312</td>\n",
       "      <td>58312</td>\n",
       "      <td>58312</td>\n",
       "      <td>58324</td>\n",
       "      <td>0</td>\n",
       "      <td>@belindayoder Aha! Now the truth comes out.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>58301</td>\n",
       "      <td>58301</td>\n",
       "      <td>58301</td>\n",
       "      <td>58313</td>\n",
       "      <td>0</td>\n",
       "      <td>@belindaaaa You love Vinny the most though. To...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>62658</td>\n",
       "      <td>62658</td>\n",
       "      <td>62658</td>\n",
       "      <td>62670</td>\n",
       "      <td>1</td>\n",
       "      <td>@bleach226 aww that's the one thing I pride my...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>58121</td>\n",
       "      <td>58121</td>\n",
       "      <td>58121</td>\n",
       "      <td>58133</td>\n",
       "      <td>0</td>\n",
       "      <td>@begiled LOL I'm a real zoo keeper with real a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>58007</td>\n",
       "      <td>58007</td>\n",
       "      <td>58007</td>\n",
       "      <td>58019</td>\n",
       "      <td>0</td>\n",
       "      <td>@beardoctor how did you know my horse didn't h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>58186</td>\n",
       "      <td>58186</td>\n",
       "      <td>58186</td>\n",
       "      <td>58198</td>\n",
       "      <td>0</td>\n",
       "      <td>@beingkris i agree!!! and then cheesecake.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>62470</td>\n",
       "      <td>62470</td>\n",
       "      <td>62470</td>\n",
       "      <td>62482</td>\n",
       "      <td>1</td>\n",
       "      <td>@BlackR0s3 Xavier is there  he asked me to com...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1     id  label_a  \\\n",
       "107       58314         58314           58314  58326        0   \n",
       "394       21962         21962           21962  21974        1   \n",
       "195       57878         57878           57878  57890        0   \n",
       "49        58146         58146           58146  58158        0   \n",
       "160       57954         57954           57954  57966        0   \n",
       "80        58332         58332           58332  58344        0   \n",
       "474       74899         74899           74899  74911        1   \n",
       "25        58233         58233           58233  58245        0   \n",
       "414       22112         22112           22112  22124        1   \n",
       "233       58080         58080           58080  58092        0   \n",
       "133       58304         58304           58304  58316        0   \n",
       "490       62720         62720           62720  62732        1   \n",
       "115       58286         58286           58286  58298        0   \n",
       "363       21955         21955           21955  21967        1   \n",
       "392       62889         62889           62889  62901        1   \n",
       "101       58357         58357           58357  58369        0   \n",
       "252       62413         62413           62413  62425        1   \n",
       "171       58003         58003           58003  58015        0   \n",
       "314       22014         22014           22014  22026        1   \n",
       "317       21997         21997           21997  22009        1   \n",
       "359       62914         62914           62914  62926        1   \n",
       "470       62777         62777           62777  62789        1   \n",
       "78        58334         58334           58334  58346        0   \n",
       "434       62644         62644           62644  62656        1   \n",
       "360       21951         21951           21951  21963        1   \n",
       "362       21947         21947           21947  21959        1   \n",
       "388       62880         62880           62880  62892        1   \n",
       "22        58246         58246           58246  58258        0   \n",
       "172       57998         57998           57998  58010        0   \n",
       "243       58123         58123           58123  58135        0   \n",
       "..          ...           ...             ...    ...      ...   \n",
       "95        58369         58369           58369  58381        0   \n",
       "401       62782         62782           62782  62794        1   \n",
       "249       58110         58110           58110  58122        0   \n",
       "44        58158         58158           58158  58170        0   \n",
       "296       62445         62445           62445  62457        1   \n",
       "264       22223         22223           22223  22235        1   \n",
       "154       57953         57953           57953  57965        0   \n",
       "298       62447         62447           62447  62459        1   \n",
       "498       62741         62741           62741  62753        1   \n",
       "451       22101         22101           22101  22113        1   \n",
       "411       62674         62674           62674  62686        1   \n",
       "173       57995         57995           57995  58007        0   \n",
       "346       62811         62811           62811  62823        1   \n",
       "37        58165         58165           58165  58177        0   \n",
       "348       22021         22021           22021  22033        1   \n",
       "439       62649         62649           62649  62661        1   \n",
       "89        58344         58344           58344  58356        0   \n",
       "343       62808         62808           62808  62820        1   \n",
       "354       62909         62909           62909  62921        1   \n",
       "319       62847         62847           62847  62859        1   \n",
       "426       62661         62661           62661  62673        1   \n",
       "262       22224         22224           22224  22236        1   \n",
       "17        58232         58232           58232  58244        0   \n",
       "127       58312         58312           58312  58324        0   \n",
       "135       58301         58301           58301  58313        0   \n",
       "445       62658         62658           62658  62670        1   \n",
       "244       58121         58121           58121  58133        0   \n",
       "167       58007         58007           58007  58019        0   \n",
       "69        58186         58186           58186  58198        0   \n",
       "263       62470         62470           62470  62482        1   \n",
       "\n",
       "                                                 tweet  subtask_a  \n",
       "107  @Beliria @vinworldwide hello friends!  Hope yo...      False  \n",
       "394  @3drik n im tryna go hoop.....wish i had a boy...       True  \n",
       "195          @beehughes118 Thank you for the follow.        False  \n",
       "49   @beatmanludmilla Thats amaizing...Thanks a lot...      False  \n",
       "160  @beamadelica ha! great stuff!  i use a few of ...      False  \n",
       "80               @beauty411 ohhhhh!  that explains it       False  \n",
       "474               @BryanRicard im football illiterate        True  \n",
       "25   @Bel @Moldor refused to let them cut his jacke...      False  \n",
       "414                          @416Jamz you replaced me        True  \n",
       "233  @BeastTheSkitzo l0l niggaaaa i dont act up i j...      False  \n",
       "133   @belindaang haha. then coffee was a good choice       False  \n",
       "490               @bikermom69  Get my email - hee hee!       True  \n",
       "115  @BelgianWaffling sniffing Neurofen plus? pray ...      False  \n",
       "363  @3btracks   I'm not performing anymore, last m...       True  \n",
       "392          @billpalmer welcome to the new facebook!        True  \n",
       "101  @beautyworks Get 100 followers a day using www...      False  \n",
       "252  @BigTastyBurger Yeah I noticed. DonkeyBiscuits...       True  \n",
       "171  @beardedbrain http://twitpic.com/50pas - u my ...      False  \n",
       "314  @_kahlaaa Yeah it is hahaha the picture is rea...       True  \n",
       "317                @_Jodi *hugs* sorry you didn't win        True  \n",
       "359  @billy_norris I bet you guys sound awesome! Wi...       True  \n",
       "470                  @blinkandyoumiss sigh. he's mean        True  \n",
       "78   @beautybanter PLEASE HELP PASS THIS ON FOR THE...      False  \n",
       "434  @blazingviolet ppps can u apologises to Fat Bu...       True  \n",
       "360                @3aneeda me too want lunch  9a7teen       True  \n",
       "362  @_jaye i'm on the fence. it's more like a help...       True  \n",
       "388  @billkaulitzlovr It was just a horrible day.  ...       True  \n",
       "22   @BeauGiles You're providing me entertainment, ...      False  \n",
       "172  @bear2care2009 Don't stick your head out of tr...      False  \n",
       "243  @begthehobo LOL, only 3?!  Ive been going insa...      False  \n",
       "..                                                 ...        ...  \n",
       "95   @bella456 G'morning! Have an awesome Aloha Fri...      False  \n",
       "401  @blip The 30 sec extracts are boring  I want t...       True  \n",
       "249  @BeffyJo You have a list of suspects?! I shoul...      False  \n",
       "44   @Beattifickid89 I can't get a break today, can...      False  \n",
       "296  @biilly &quot;Sorry, currently our video libra...       True  \n",
       "264             @_meeks_ Omg don't scare me like that        True  \n",
       "154                      @beam_orangery few = phew...       False  \n",
       "298  @BlackMamba23 you included people you don't ev...       True  \n",
       "498  @Bill_Cameron I know, just trying to look on t...       True  \n",
       "451        @3zlyca Hope your back feels better today.        True  \n",
       "411  @bleedingxsoul He said that he thought I was t...       True  \n",
       "173        @bear_eyes ) Are we still up for tomorrow?       False  \n",
       "346  @blkwithwhtstrpz lmao oh yea! Hmmm...this celi...       True  \n",
       "37   @BeaucoupKevin what is a decent blog software/...      False  \n",
       "348  @3LL3N it was the best day! idunno where it is...       True  \n",
       "439       @Blazulka96 This is interesting  I am bored        True  \n",
       "89                                   @beautyfulashley       False  \n",
       "343                      @BLKPARISHILTON1 mine either        True  \n",
       "354  @Billy I stayed up all night listening to &quo...       True  \n",
       "319  @BillBubbaBussey Rick is still old school text...       True  \n",
       "426                                  @BleacherBabble         True  \n",
       "262  @_meeks_ OMG! i think something is wrong with ...       True  \n",
       "17   @BeKultured I try my best. Sometimes with succ...      False  \n",
       "127       @belindayoder Aha! Now the truth comes out.       False  \n",
       "135  @belindaaaa You love Vinny the most though. To...      False  \n",
       "445  @bleach226 aww that's the one thing I pride my...       True  \n",
       "244  @begiled LOL I'm a real zoo keeper with real a...      False  \n",
       "167  @beardoctor how did you know my horse didn't h...      False  \n",
       "69         @beingkris i agree!!! and then cheesecake.       False  \n",
       "263  @BlackR0s3 Xavier is there  he asked me to com...       True  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test_DF.sample(frac=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.to_csv('../Dataset-TSA/sentiment500.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets500 = tweets_test_DF[:500]\n",
    "#tweets500.to_csv('../Dataset-TSA/sentiment500.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweets_test_DF = tweets_test_DF.rename(columns={\"SentimentText\":\"tweet\", \"Sentiment\":\"label_a\", \"ItemID\":\"id\"})\\ntweets_test_DF[\"Unnamed: 0\"] = tweets_test_DF.index\\ntweets_test_DF.to_csv(\\'../Dataset-TSA/sentiment.csv\\', encoding=\\'utf-8\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing new data\n",
    "'''\n",
    "tweets_test_DF = tweets_test_DF.rename(columns={\"SentimentText\":\"tweet\", \"Sentiment\":\"label_a\", \"ItemID\":\"id\"})\n",
    "tweets_test_DF[\"Unnamed: 0\"] = tweets_test_DF.index\n",
    "tweets_test_DF[\"label_a\"] = tweets_test_DF[\"label_a\"].apply(lambda x: int(not x))\n",
    "tweets_test_DF = tweets_test_DF.drop(columns=[\"Unnamed: 0.1\"], axis=1)\n",
    "tweets_test_DF[\"subtask_a\"] = tweets_test_DF[\"label_a\"].apply(lambda x: True if x else False)\n",
    "tweets_test_DF.to_csv('../Dataset-TSA/sentiment.csv', encoding='utf-8')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import PorterStemmer\\nfrom nltk.tokenize import WordPunctTokenizer\\nfrom nltk.collocations import BigramCollocationFinder\\nfrom nltk.metrics import BigramAssocMeasures\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessery columns\n",
    "tweets_DF = tweets_DF.rename(columns = {'label_a':'label'})\n",
    "tweets_DF = tweets_DF.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "\n",
    "tweets_test_DF = tweets_test_DF.rename(columns = {'label_a':'label'})\n",
    "tweets_test_DF = tweets_test_DF.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------\n",
    "\n",
    "Removing @USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_USER = re.compile(\"@USER\")\n",
    "\n",
    "def remove_user(tweet):\n",
    "    return REMOVE_USER.sub(\"\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['tweet'] = tweets_DF['tweet'].apply(remove_user)\n",
    "tweets_test_DF['tweet'] = tweets_test_DF['tweet'].apply(remove_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "Extracting emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['emoji'] = tweets_DF['tweet'].apply(split_count)\n",
    "tweets_test_DF['emoji'] = tweets_test_DF['tweet'].apply(split_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return [score['neg'], score['neu'], score['pos'], score['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['sentiment'] = tweets_DF['tweet'].apply(sentiment_analyzer_scores)\n",
    "tweets_DF['negative_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[0])\n",
    "tweets_DF['neutral_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[1])\n",
    "tweets_DF['positive_sentiment'] = tweets_DF['sentiment'].apply(lambda row: row[2])\n",
    "tweets_DF['compound'] = tweets_DF['sentiment'].apply(lambda row: row[3])\n",
    "\n",
    "tweets_DF = tweets_DF.drop([\"sentiment\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_DF['sentiment'] = tweets_test_DF['tweet'].apply(sentiment_analyzer_scores)\n",
    "tweets_test_DF['negative_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[0])\n",
    "tweets_test_DF['neutral_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[1])\n",
    "tweets_test_DF['positive_sentiment'] = tweets_test_DF['sentiment'].apply(lambda row: row[2])\n",
    "tweets_test_DF['compound'] = tweets_test_DF['sentiment'].apply(lambda row: row[3])\n",
    "\n",
    "tweets_test_DF = tweets_test_DF.drop([\"sentiment\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### C) Positive, negative and offensive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words_file = open(\"negative-words.txt\")\n",
    "negative_words = []\n",
    "\n",
    "positive_words_file = open(\"positive-words.txt\")\n",
    "positive_words = []\n",
    "\n",
    "for line in negative_words_file:\n",
    "    if not str.startswith(line, \";\"):\n",
    "        negative_words.append(line.split(\"\\n\")[0])\n",
    "        \n",
    "negative_words = negative_words[1:]\n",
    "\n",
    "for line in positive_words_file:\n",
    "    if not str.startswith(line, \";\"):\n",
    "        positive_words.append(line.split(\"\\n\")[0])\n",
    "        \n",
    "positive_words = positive_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_negative_words(tweet):\n",
    "    tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "    if any(word in negative_words for word in words):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_for_positive_words(tweet):\n",
    "    tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "    if any(word in positive_words for word in words):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['negative_words'] = tweets_DF['tweet'].apply(check_for_negative_words)\n",
    "tweets_test_DF['negative_words'] = tweets_test_DF['tweet'].apply(check_for_negative_words)\n",
    "\n",
    "tweets_DF['positive_words'] = tweets_DF['tweet'].apply(check_for_positive_words)\n",
    "tweets_test_DF['positive_words'] = tweets_test_DF['tweet'].apply(check_for_positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offensive_words_file = open(\"facebook_bad_words.txt\")\n",
    "offensive_words = []\n",
    "\n",
    "for line in offensive_words_file:\n",
    "    offensive_words.append(line.strip())\n",
    "        \n",
    "offensive_words[0] in \"baldjd 2g1c\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_offensive_words(tweet):\n",
    "    #tweet = preprocess_tweet(tweet)\n",
    "    tweet = str.lower(tweet)\n",
    "    if any(word in tweet for word in offensive_words):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['offensive_words'] = tweets_DF['tweet'].apply(check_for_offensive_words)\n",
    "tweets_test_DF['offensive_words'] = tweets_test_DF['tweet'].apply(check_for_offensive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "### D) Removing stopwords, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #SnowballStemmer('english')\n",
    "   \n",
    "def preprocess_tweet(tweet, remove_stopwords=False):\n",
    "    #remove all non letters\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    removed_nonalphanumeric = regex.sub(' ', tweet)\n",
    "    lowercased_tweet = str.lower(removed_nonalphanumeric)\n",
    "\n",
    "    #remove stopwords\n",
    "    if remove_stopwords:\n",
    "        lowercased_tweet = \" \".join(word if word not in english_stop_words else \"\" for word in lowercased_tweet.split())\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in lowercased_tweet.split()])\n",
    "    return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT \n",
      "\n",
      "amazon is investigating chinese employee who are selling internal data to third party seller looking for an edge in the competitive marketplace url amazon maga kag china tcot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'amazon investigating chinese employee selling internal data third party seller looking edge competitive marketplace url amazon maga kag china tcot'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets_DF['tweet'][2]\n",
    "print(tweet, \"\\n\")\n",
    "print(preprocess_tweet(tweet, False))\n",
    "preprocess_tweet(tweet, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['clean_tweet_with_stopwords'] = tweets_DF['tweet'].apply(preprocess_tweet, False)\n",
    "tweets_DF['clean_tweet_without_stopwords'] = tweets_DF['tweet'].apply(preprocess_tweet, True)\n",
    "\n",
    "tweets_test_DF['clean_tweet_with_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet, False)\n",
    "tweets_test_DF['clean_tweet_without_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, word2vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tweets_DF['clean_tweet_with_stopwords'].values\n",
    "\n",
    "# we need to pass splitted sentences to the model\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "#model.train(corpus, total_examples=1000, epochs=10)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd(sentance):\n",
    "    tokens = sentance.split()\n",
    "    summed_embedding = np.zeros(100)\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            summed_embedding += model.wv[token]\n",
    "        \n",
    "        \n",
    "    return summed_embedding/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz = TweetTokenizer()\n",
    "tweets_test_DF['clean_tweet_with_stopwords'] = tweets_test_DF['tweet'].apply(preprocess_tweet)#(tknz.tokenize).apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF['embedding'] = tweets_DF['clean_tweet_with_stopwords'].apply(embedd)\n",
    "tweets_test_DF['embedding'] = tweets_test_DF['clean_tweet_with_stopwords'].apply(embedd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models\n",
    "------------------------------------\n",
    "\n",
    "### 2.1 Model: sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train, test \n",
    "sentiment = tweets_DF[[\"negative_sentiment\", \"negative_words\", \"offensive_words\"]].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "X_test_sentiment = tweets_test_DF[[\"negative_sentiment\", \"negative_words\", \"offensive_words\"]].values\n",
    "y_test_sentiment = tweets_test_DF['label'].values\n",
    "\n",
    "X_train_sentiment, X_val_sentiment, y_train_sentiment, y_val_sentiment = train_test_split(sentiment, y, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.492\n"
     ]
    }
   ],
   "source": [
    "classifier_sentiment = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "classifier_sentiment.fit(X_train_sentiment, y_train_sentiment)\n",
    "score = classifier_sentiment.score(X_test_sentiment, y_test_sentiment)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4065420560747664"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_sentiment.predict(X_test_sentiment)\n",
    "f1_score(y_test_sentiment, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model : tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['tweet'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['tweet'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "vectorizer_tf = TfidfVectorizer()\n",
    "vectorizer_tf.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_tfidf = vectorizer_tf.transform(tweets_train)\n",
    "X_valid_tfidf = vectorizer_tf.transform(tweets_valid)\n",
    "X_test_tfidf  = vectorizer_tf.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.362\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = LogisticRegression(max_iter=100)\n",
    "classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "score = classifier_tfidf.score(X_test_tfidf, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1212121212121212"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_tfidf.predict(X_test_tfidf)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model: count_vector, clean data: char-based and word-based\n",
    "- Old data voc size: 19083\n",
    "- New data voc size: 16622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['clean_tweet_without_stopwords'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['clean_tweet_without_stopwords'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "char_vectorizer = CountVectorizer(binary=True, analyzer='char' , ngram_range=(3, 5))\n",
    "char_vectorizer.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_clean = char_vectorizer.transform(tweets_train)\n",
    "X_valid_clean = char_vectorizer.transform(tweets_valid)\n",
    "X_test_clean  = char_vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.404\n"
     ]
    }
   ],
   "source": [
    "classifier_char = LogisticRegression(max_iter=1000)\n",
    "classifier_char.fit(X_train_clean, y_train)\n",
    "score = classifier_char.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2836538461538461"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_char.predict(X_test_clean)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(y_test)\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['clean_tweet_without_stopwords'].values\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['clean_tweet_without_stopwords'].values\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,  ngram_range=(1, 3))\n",
    "vectorizer.fit(tweets)\n",
    "\n",
    "tweets_train, tweets_valid, y_train, y_valid = train_test_split(tweets, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "X_train_clean = vectorizer.transform(tweets_train)\n",
    "X_valid_clean = vectorizer.transform(tweets_valid)\n",
    "X_test_clean  = vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.366\n"
     ]
    }
   ],
   "source": [
    "classifier_clean = LogisticRegression(max_iter=1000)\n",
    "classifier_clean.fit(X_train_clean, y_train)\n",
    "score = classifier_clean.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14092140921409213"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_clean.predict(X_test_clean)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599056603773585\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = vectorizer.transform(tweets[:9000])\n",
    "X_test_clean  = vectorizer.transform(tweets[9000:])\n",
    "y_train = y[:9000]\n",
    "y_test = y[9000:]\n",
    "\n",
    "classifier_clean = LogisticRegression(max_iter=1000)\n",
    "classifier_clean.fit(X_train_clean, y_train)\n",
    "score = classifier_clean.score(X_test_clean, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed to predicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier_clean.predict(X_test_clean)\n",
    "z = zip(prediction, y_test, range(9000, 9000+len(prediction)))\n",
    "indices = np.array([i if p1!=p2 else -1 for p1, p2, i in z ])\n",
    "indices = indices[indices>0]\n",
    "for i in indices:\n",
    "    print(tweets_DF['label'][i],tweets_DF['tweet'][i], tweets_DF['negative_words'][i],  \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_DF['embedding'].values\n",
    "tweets = np.array([np.array(t) for t in tweets]).reshape(-1, 100)\n",
    "y = tweets_DF['label'].values\n",
    "\n",
    "tweets_test = tweets_test_DF['embedding'].values\n",
    "X_test_embedding = np.array([np.array(t) for t in tweets_test]).reshape(-1, 100)\n",
    "y_test = tweets_test_DF['label'].values\n",
    "\n",
    "X_train_embedding, X_val_embedding, y_train, y_val = train_test_split(tweets, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.326\n"
     ]
    }
   ],
   "source": [
    "classifier_embedding = LogisticRegression(max_iter=1000)\n",
    "classifier_embedding.fit(X_train_embedding, y_train)\n",
    "score = classifier_embedding.score(X_test_embedding, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier_embedding.predict(X_test_embedding)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansamble(sentence):\n",
    "    \n",
    "    sentence = remove_user(sentence)\n",
    "    neg, neu, pos, comp =  sentiment_analyzer_scores(sentence)\n",
    "    negative_words = check_for_negative_words(sentence)\n",
    "    offensive_words = check_for_offensive_words(sentence)\n",
    "    \n",
    "    preproc_with_stopwords = preprocess_tweet(sentence)\n",
    "    preproc_no_stopwords = preprocess_tweet(sentence, True)\n",
    "    \n",
    "    #emb1 = embedd(preproc_with_stopwords)\n",
    "    #emb2 = embedd(preproc_no_stopwords)\n",
    "    \n",
    "    score1 = classifier_sentiment.predict([[neg, negative_words, offensive_words]])\n",
    "    \n",
    "    tfidf = vectorizer_tf.transform([sentence])\n",
    "    score2 = classifier_tfidf.predict(tfidf)\n",
    "    \n",
    "    count = vectorizer.transform([preproc_no_stopwords])\n",
    "    score3 = classifier_clean.predict(count)\n",
    "    \n",
    "    count = char_vectorizer.transform([preproc_no_stopwords])\n",
    "    score4 = classifier_char.predict(count)\n",
    "    \n",
    "    #score5 = classifier_embedding.predict([emb1])\n",
    "    \n",
    "    if not any(w in model.wv for w in preproc_no_stopwords.split()):# and score4[0]:\n",
    "            return score4[0]\n",
    "    \n",
    "    vote = collections.Counter([score1[0], score2[0], score3[0], score4[0], score3[0]])\n",
    "    if vote[0] > 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382\n",
      "0.8421052631578947\n",
      "0.0955223880597015\n",
      "0.17158176943699732\n"
     ]
    }
   ],
   "source": [
    "test_tweets = tweets_test_DF['tweet'].values\n",
    "y = tweets_test_DF['label'].values\n",
    "\n",
    "y_pred = [ansamble(t) for t in test_tweets]\n",
    "\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(precision_score(y, y_pred))\n",
    "print(recall_score(y, y_pred))\n",
    "print(f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Test sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "    label = [\"NOT\", \"OFF\"]\n",
    "    \n",
    "    sentence = remove_user(sentence)\n",
    "    neg, neu, pos, comp =  sentiment_analyzer_scores(sentence)\n",
    "    negative_words = check_for_negative_words(sentence)\n",
    "    offensive_words = check_for_offensive_words(sentence)\n",
    "    \n",
    "    preproc_with_stopwords = preprocess_tweet(sentence)\n",
    "    preproc_no_stopwords = preprocess_tweet(sentence, True)\n",
    "    \n",
    "    emb1 = embedd(preproc_with_stopwords)\n",
    "    emb2 = embedd(preproc_no_stopwords)\n",
    "    \n",
    "    score1 = classifier_sentiment.predict([[neg, negative_words, offensive_words]])\n",
    "    print(\"Features:    Classified as:\", label[score1[0]])\n",
    "    \n",
    "    tfidf = vectorizer_tf.transform([sentence])\n",
    "    score2 = classifier_tfidf.predict(tfidf)\n",
    "    print(\"TF_idf:      Classified as:\", label[score2[0]])\n",
    "    \n",
    "    count = vectorizer.transform([preproc_no_stopwords])\n",
    "    score3 = classifier_clean.predict(count)\n",
    "    print(\"Ngrams:      Classified as:\", label[score3[0]])\n",
    "    \n",
    "    count = char_vectorizer.transform([preproc_no_stopwords])\n",
    "    score4 = classifier_char.predict(count)\n",
    "    print(\"Char-ngrams: Classified as:\", label[score4[0]])\n",
    "    \n",
    "    score5 = classifier_embedding.predict([emb1])\n",
    "    print(\"Embeddings:  Classified as:\", label[score5[0]])\n",
    "    \n",
    "    score6 = ansamble(sentence)\n",
    "    print(\"Ansamble:    Classified as:\", label[score6])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"Are you fuckisiiifn in?\")\n",
    "a = \"Are you fuckisiiifn in?\"\n",
    "any([w in model.wv for w in a.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## 4. Testing correlation\n",
    "__Tested correlation of some features and labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tweets_DF['label'].values\n",
    "y_negative_words = tweets_DF['negative_words'].values\n",
    "y_offensive_words = tweets_DF['offensive_words'].values\n",
    "y_negative_sentiment = tweets_DF['negative_sentiment'].values\n",
    "y_negative_sentiment = [1 if neg > 0  else 0 for neg in y_negative_sentiment]\n",
    "\n",
    "print(\"Tweet contains negative word: \", matthews_corrcoef(y_pred, y_negative_words))\n",
    "print(\"Tweet has some negative sentiment: \", matthews_corrcoef(y_pred, y_negative_sentiment))\n",
    "print(\"Tweet contains offensive words: \", matthews_corrcoef(y_pred, y_offensive_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "https://docs.google.com/document/d/1OdniS8GEYwaFJy_zNC5GpXkceuGxmOtPK_dV9QAecho/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
