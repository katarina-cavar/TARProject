{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLID Clean Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kcava\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import argparse\n",
    "\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, folder=\"../Dataset-OLID/OLIDv1.0/\", \n",
    "                 task_a=\"data_subtask_a.csv\"):\n",
    "        self.folder = folder\n",
    "        self.task_a = task_a\n",
    "        \n",
    "    def get_df_train_data(self):\n",
    "        train_data = pd.read_csv(self.folder + self.task_a)\n",
    "        train_tweets = train_data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_df_data(self, file=\"data_subtask_a.csv\"):\n",
    "        data = pd.read_csv(self.folder + file)\n",
    "        train_tweets = data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "        return train_tweets\n",
    "    \n",
    "    def get_np_data_and_labels(self, file=\"data_subtask_a.csv\"):\n",
    "        tweets = self.get_df_data(file)\n",
    "        data, labels = tweets.values[:,0], tweets.values[:,1]\n",
    "        return data, labels\n",
    "    \n",
    "    # this creates copies\n",
    "    def shuffle_np(self, data, labels):\n",
    "        assert len(data) == len(labels)\n",
    "        p = np.random.permutation(len(data))\n",
    "        return data[p], labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def no_preprocessing(self, data, verbose=False):\n",
    "        return data\n",
    "\n",
    "    def remove_punctuation(self, data, verbose=False):\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            data[i] = sentence.copy()\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"’\")\n",
    "        \n",
    "        tknzr = TweetTokenizer()\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            #sentence_blob = TextBlob(data[i])\n",
    "            sentence_blob = tknzr.tokenize(data[i])\n",
    "            #print(\"Blob: \", sentence_blob)\n",
    "            sentence = \" \".join(sentence_blob) #.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            #words = data[i].split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n",
    "    \n",
    "    def remove_stopwords_and_punctuation_textblob(self, data, verbose = False):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop.append(\"’\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(type(stop))\n",
    "            print(stop)\n",
    "        noise = [\"user\"]\n",
    "        for i in range(len(data)):\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            sentence_blob = TextBlob(data[i])\n",
    "            sentence = \" \".join(sentence_blob.words)\n",
    "            #print(sentence)\n",
    "            words = sentence.split()\n",
    "            \n",
    "            #Remove stopwords\n",
    "            if verbose:\n",
    "                print(words)\n",
    "            clean_words = []\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip().lower()\n",
    "                if verbose:\n",
    "                    print(word)\n",
    "                if word not in stop: \n",
    "                    clean_words.append(word)\n",
    "                else: \n",
    "                    if verbose:\n",
    "                        print(\"Remove: \", word)\n",
    "            \n",
    "            data[i] = \" \".join(clean_words)\n",
    "            if verbose:\n",
    "                print(data[i])\n",
    "                print(\"-\"*20)\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(messages, labels, args):\n",
    "\n",
    "    VOCAB_SIZE = args.vocab_size\n",
    "    MAX_LENGTH = args.max_len\n",
    "    TRUNC_TYPE = args.trunc_type\n",
    "    PADDING_TYPE = args.pad_type\n",
    "    OOV_TOK = args.oov_tok\n",
    "    TRAINING_PORTION = args.train_portion\n",
    "\n",
    "    train_number = int(len(messages) * TRAINING_PORTION)\n",
    "\n",
    "    train_msgs = messages[:train_number]\n",
    "    train_labels = labels[:train_number]\n",
    "    val_msgs = messages[train_number:]\n",
    "    val_labels = labels[train_number:]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token = OOV_TOK)\n",
    "    tokenizer.fit_on_texts(train_msgs)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\"len(msgs) = {}; len(labels) = {}\".format(len(messages), len(labels)))\n",
    "    print(\"TRAIN: len(x) = {}; len(y) = {}\".format(len(train_msgs),len(train_labels)))\n",
    "    print(\"TEST: len(x) = {}; len(y) = {}\".format(len(val_msgs),len(val_labels)))\n",
    "\n",
    "\n",
    "    print(\"\\nlen(word_index) = {}\\n\".format(len(word_index))) \n",
    "    # Total number of words without stopwords = 8029\n",
    "    #print(word_index)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_msgs)\n",
    "    train_padded = pad_sequences(train_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    val_sequences = tokenizer.texts_to_sequences(val_msgs)\n",
    "    val_padded = pad_sequences(val_sequences, maxlen = MAX_LENGTH, \n",
    "                                padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n",
    "\n",
    "    print(type(val_padded))\n",
    "    \n",
    "    return train_padded, train_labels, val_padded, val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model(args):\n",
    "    VOCAB_SIZE = args.vocab_size\n",
    "    EMBEDDING_DIM = args.emb_dim\n",
    "    MAX_LENGTH = args.max_len\n",
    "    NUM_EPOCHS = args.num_epochs\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
