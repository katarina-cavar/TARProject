{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Deep Learning Networks on OLID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "**Labels:**\n",
    "- OFF = Offensive = 1 \n",
    "- NOT = Not Offensive = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              tweet  \\\n",
       "0           0  86426  @USER She should ask a few native Americans wh...   \n",
       "1           1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...   \n",
       "2           2  16820  Amazon is investigating Chinese employees who ...   \n",
       "3           3  62688  @USER Someone should'veTaken\" this piece of sh...   \n",
       "4           4  43605  @USER @USER Obama wanted liberals &amp; illega...   \n",
       "5           5  97670                  @USER Liberals are all Kookoo !!!   \n",
       "6           6  77444                   @USER @USER Oh noes! Tough shit.   \n",
       "7           7  52415  @USER was literally just talking about this lo...   \n",
       "8           8  45157                         @USER Buy more icecream!!!   \n",
       "9           9  13384  @USER Canada doesn’t need another CUCK! We alr...   \n",
       "\n",
       "  subtask_a  label_a  \n",
       "0       OFF        1  \n",
       "1       OFF        1  \n",
       "2       NOT        0  \n",
       "3       OFF        1  \n",
       "4       NOT        0  \n",
       "5       OFF        1  \n",
       "6       OFF        1  \n",
       "7       OFF        1  \n",
       "8       NOT        0  \n",
       "9       OFF        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('../Dataset-OLID/OLIDv1.0/data_subtask_a.csv')\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label_a\n",
       "0  @USER She should ask a few native Americans wh...        1\n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...        1\n",
       "2  Amazon is investigating Chinese employees who ...        0\n",
       "3  @USER Someone should'veTaken\" this piece of sh...        1\n",
       "4  @USER @USER Obama wanted liberals &amp; illega...        0\n",
       "5                  @USER Liberals are all Kookoo !!!        1\n",
       "6                   @USER @USER Oh noes! Tough shit.        1\n",
       "7  @USER was literally just talking about this lo...        1\n",
       "8                         @USER Buy more icecream!!!        0\n",
       "9  @USER Canada doesn’t need another CUCK! We alr...        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = raw_data.drop([\"Unnamed: 0\", \"id\", \"subtask_a\"], axis=1)\n",
    "train_tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Data Science: [Twitter Sentiment Analysis using NLTK, Python](https://towardsdatascience.com/twitter-sentiment-analysis-classification-using-nltk-python-fa912578614c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x148c3148708>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label_a', data=train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Feature Engineering\n",
    "\n",
    "- removal of punctuations\n",
    "- removal of commonly used words (stopwords)\n",
    "- normalization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kcava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords','punkt', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  USER USER USER It ’ s not my fault you support gun control\n",
      "Original:  @USER @USER @USER It’s not my fault you support gun control\n"
     ]
    }
   ],
   "source": [
    "# Punctuations\n",
    "\n",
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "print(\"Processed: \", form_sentence(train_tweets['tweet'].iloc[10]))\n",
    "print(\"Original: \", train_tweets['tweet'].iloc[10])\n",
    "\n",
    "\n",
    "#â ireland consumer price index mom climbed from previous 0.2 to 0.5 in may blog silver gold forex\n",
    "#â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USER', 'USER', 'USER', 'fault', 'support', 'gun', 'control']\n",
      "@USER @USER @USER It’s not my fault you support gun control\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "\n",
    "def no_user_alpha(tweet):\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean_mess\n",
    "print(no_user_alpha(form_sentence(train_tweets['tweet'].iloc[10])))\n",
    "print(train_tweets['tweet'].iloc[10])\n",
    "\n",
    "\n",
    "#['ireland', 'consumer', 'price', 'index', 'mom', 'climbed', 'previous', 'may', 'blog', 'silver', 'gold', 'forex']\n",
    "#â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'play', 'with', 'my', 'friends', 'with', 'whom', 'I', 'use', 'to', 'play,', 'when', 'you', 'call', 'me', 'yesterday']\n",
      "['I', 'was', 'playing', 'with', 'my', 'friends', 'with', 'whom', 'I', 'used', 'to', 'play,', 'when', 'you', 'called', 'me', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "# Lexicon Normalization\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "tweet_list = 'I was playing with my friends with whom I used to play, when you called me yesterday'.split()\n",
    "print(normalization(tweet_list))\n",
    "print(tweet_list)\n",
    "\n",
    "#['I', 'be', 'play', 'with', 'my', 'friends', 'with', 'whom', 'I', 'use', 'to', 'play,', 'when', 'you', 'call', 'me', 'yesterday']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization and Model Selection\n",
    "- we have to numerically represent the preprocessed data\n",
    "- techniques for vectorization of words: \n",
    "    - count vefctorization\n",
    "    - Tf-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_processing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-adadefdbd3cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m pipeline = Pipeline([\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'bow'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_processing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# strings to token integer counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# integer counts to weighted TF-IDF scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# train on TF-IDF vectors w/ Naive Bayes classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_processing' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(train_tweets['tweet'], train_tweets['label'], test_size=0.2)\n",
    "pipeline.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "print(classification_report(predictions,label_test))\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
